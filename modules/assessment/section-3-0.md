---
layout: module
title: 3.0 Evaluation
module_name: assessment
module_full_name: Assessment and Evaluation
section: 2. Evaluation
subsection: three-zero
---

In this introduction, you will learn what mentoring is and how it can support connected learning in the library.

## Learning Outcomes For This Section

**After completing Section 3: Evaluation, you will be able to...**
<ul class="fancy">
  <li>Identify the stakeholders for your evaluation</li>
  <li>Develop evaluation questions</li>
  <li>Use assessments to produce data and evidence to answer your evaluation question </li>
  <li>Write an evaluation plan</li>
</ul>

>"Evaluation is about judging the value of an initiative.” -- Dr. William Penuel (University of Colorado at Boulder)<sup>[43](#fn4)</sup> 

Much of the time, your information needs can be met with one or two assessments. Other times, you have a bigger, deeper question that an assessment can address. In these cases, you can perform an evaluation. An evaluation makes a judgment about something, such as the value, quality, or efficacy of a program, partnership, or other activity, to provide information that will help with a decision about some future action.<sup>[5](#fn5)</sup>  It begins with questions, finds answers, and ends with conclusions or recommendations. The questions you have will help determine the type of evaluation you choose and how you use it. As you design your evaluation, think about the following factors: 

- Why are you conducting an evaluation?  
- Who are you doing it for?  
- How do you want the findings to be used? 

<div class="case_study_box">  

<p><span class="box-title">UTILIZATION-FOCUSED EVALUATION</span></p> 

<p style="text-align:center">
  The utilization-focused approach to evaluation maintains that an evaluation is only as good as it is useful. Design your evaluation in a way that will maximize its usefulness to its intended audience—whether that is your manager, a partner, or yourself. Learn more at <a href="http://www.betterevaluation.org/en/plan/approach/utilization_focused_evaluation">BetterEvaluation.org.</a></p>  

</div> 

<br>

### Stakeholders

An evaluation can have several stakeholders. After yourself, the most prominent group of stakeholders is the audience for the final report—often composed of managers, funders, or other authority figures. Of course, they are not your only stakeholders and may not even be the most important ones. Other stakeholders may include:  

- Other library staff who worked on the project or who may work on the project (or similar projects) in the future  
- Any partners who worked with you  
- Participants in the program, as well as their parents. 
- Other community members who are impacted by the initiative.  
- People who may want to participate in the future.  

Your stakeholders should be involved in the evaluation process—defining values, identifying essential measures, and collecting or providing data. They may have different perspectives on what the outcomes should be and ways to measure them. You may not be able to speak to every individual stakeholder, but try to get input from representatives of each group, at least. However, don’t fall into the trap of thinking you have to follow every recommendation from every group—as the saying goes, if you try to please everyone, you end up pleasing no one.<sup>[6](#fn6)</sup> The purpose of an evaluation is to inform a decision, not dictate the path forward.<sup>[7](#fn7)</sup> 

The answer to the question should be something one or more of your stakeholder groups can use to make a decision (such as continuing to fund a program or ending a partnership). 

<div class="case_study_box">  

<p><span class="box-title">PARTICIPATORY EVALUATION</span></p> 

<p style="text-align:center">
  Stakeholders should always be involved in an evaluation, providing context, advice, and feedback about the evaluation. Participatory evaluation takes stakeholder participation to the next level and actively involves stakeholders in the process of planning and carrying out the evaluation. To learn more about participatory evaluation, see Chapter 36, Section 2 of the Community Toolbox: <a href="http://ctb.ku.edu/en/table-of-contents/evaluate/evaluation/intervention-research/main">Community-Based Participatory Research.</a></p>  

</div> 

<br>

### Outcomes

Outcomes are the differences your programs make for your library’s youth and for the community—things outside of your program and outside of connected learning that matter.<sup>[8](#fn8)</sup> If you are evaluating a program or other connected learning activity, you may have already determined the desired outcomes (see the Connected Learning Programming module for more).  

<div class="case_study_box">  

<p><span class="box-title">OUTCOME EXAMPLES</span></p> 

<p style="text-align:center">
  <i>Participants will want to learn more about robotics</i><br>

  <i>Participants will develop collaboration skills</i>
</p>  

</div> 

<br>


### Evaluation questions

You probably have some idea of why you are conducting an evaluation—possibly one of the reasons listed in What is an evaluation? For instance, evaluations can directly affect future actions or decisions or change someone’s attitude or way of thinking.<sup>[9](#fn9)</sup> Evaluation questions transform a vague idea into specific elements that are tied to one or more outcomes and can be measured through assessments. 

All evaluations answer the same three fundamental questions, according to the <a href="http://www.informalscience.org/evaluation/pi-guide">Principal Investigator’s Guide:</a><sup>[10](#fn10)</sup>

1. What? (What happened or happens and what are the results?) 
2. So what? (Why did the results occur the way they did, and what are the implications?) 
3. Now what? (What actions, decisions, or recommendations can be made based on the results?)

<table>
  <tr><th>DON'T</th><th>DO</th></tr>
  <tr><td style="background-color:#CAC9C9">Ask more questions than you have time or resources to answer</td><td style="background-color:#F6DEB7">Prioritize questions based on the value of their answers and your ability to answer them<sup>11</sup></td></tr>
  <tr><td style="background-color:#CAC9C9">Come up with all the questions yourself</td><td style="background-color:#FCFB9D">Talk to stakeholders for their input</td></tr>
  <tr><td style="background-color:#CAC9C9">Try to answer all questions from every stakeholder</td><td style="background-color:#CCEDC3">Use stakeholder input to <i>inform</i> the evaluation design, not <i>determine</i> it </td></tr>
  <tr><td style="background-color:#CAC9C9">Ask questions that are too broad or aspirational</td><td style="background-color:#D8C5E1">Ask questions that can be answered realistically</td></tr>
</table>

</div> 

<br>

 

<div class="case_study_box">  

<p><span class="box-title">EVALUATION QUESTION EXAMPLES</span></p> 

<p style="text-align:center">
  <i>How financially sustainable is the robotics lab? </i><br>

  <i>Does the robotics lab help teens develop collaboration skills?</i>
</p>  

</div> 

<br>

<table class="basic"> 

  <tr><th style="width:33%">EVALUATION TYPE</th><th style="width:33%">DESCRIPTION</th><th style="width:33%">EXAMPLES</th></tr> 

  <tr><td>Front-end evaluation</td><td>Used in informal science education; similar to audience or user research and used to inform the initial design of a program.<sup>12</sup></td><td>What aspects of this topic are our youth interested in?<br><br>What is our audience’s current skill level?</td></tr> 

  <tr><td>Formative evaluation</td><td>Periodic evaluations conducted during the development and implementation of a program. Used for course-correcting if necessary. Formative evaluation can look at how a project is progressing towards its goals whether the implementation of the project going according to plan.<sup>13</sup> </td><td>Are participants learning what we expected?<br><br>Are there any issues with implementation of the program?</td></tr> 

  <tr><td>Summative evaluation</td><td>Focuses on conditions at the end of the program and is an evaluation of the entire process. It may report results from earlier formative evaluations as well. </td><td>Did participants learn what and as much as we wanted?<br><br>What was the value of the partnership?</td></tr> 

</table>

### Using assessments in evaluations

Assessments used in evaluations are not much different from the standalone assessments discussed in Section 2. Since evaluations ask deep and complex questions, however, they typically involve multiple assessments to help build a better picture and construct stronger evidence. 

#### Indicators 

Assessments used in evaluations need to be tied directly to the outcome or outcomes you are evaluating. You need to choose indicators that will provide measurable ways to show the impact of your initiative in relation to that outcome. To determine the appropriate indicators for your project, look at one of your outcomes and ask “How will I know it when I see it?” <sup>[14](#fn14)</sup>  

Just as the outcomes of connected learning are often nontraditional, so are the indicators. Using a variety of indicators to answer a single question can help triangulate your interpretations. (The same indicator can also be used to answer multiple questions sometimes.) Traditional library assessment measures (number of attendees, number of questions asked, how often a resource is used, patron satisfaction) play an important role and can be part of your evaluation. However, in a connected learning evaluation, they should exist in support of other measures. 

<div class="tips">  

<p><span class="box-title">DON’T START WITH INDICATORS</span></p> 
<p><b>Don’t start with indicators!</b> Start by determining your desired outcomes. If you start with measures, you run the risk of defaulting to only the easiest and most obvious measures.15 The outcomes of connected learning can be much richer than that. Start with the outcomes, and then figure out the best way to measure your progress towards those outcomes</p>

### Evaluation plans 

An evaluation plan is a roadmap for the activities and assessments you will conduct to answer your questions. If your evaluation plan will be used only internally, you may not need every section. If you are working with a partner, make sure you are both onboard with the plan. 

#### Overview and Background Information  

If you will be sharing it with people who aren’t deeply familiar with the project, introduce the project and the reason for the evaluation. After reading these sections, your readers should have a general sense of what is being evaluated and why.  

- **Executive summary**. An abstract of the entire report. This may be the only section that some of your stakeholders read, so capture the “gist” of the evaluation clearly and succinctly. You can also write different summaries for different stakeholders.  
- **Project (or program, initiative, etc.) description**. Include a high-level overview, then go into detail about the elements or activities that make it up. Include any history or context that might be helpful.   
- **Purpose of the evaluation**. Describe the general reason you are conducting an evaluation and what you hope to get out of it. A detailed examination of your evaluation questions comes later, so don’t get too granular here.  
- **Key stakeholders**. Introduce your readers to the key people involved with or affected by your project (see Section X). You can also introduce key project staff here.  

#### Program Outcomes 

These sections explain in greater detail what your program does, and how and why you expect it to achieve what you want.  

- **Program goals**. Introduce the impacts and outcomes you hope your project will have.  
- **Theory of change or logic model**. This section explains “how and why the program will work”16 in both narrative and graphic formats. (For more about developing theories of change, see the Connected Learning Programs module.) 
- **Activities**. Include a list of all the activities that will take place as part of the project being evaluated. You may break them into categories—e.g., activities that staff will complete, activities the partner will complete, and activities the program participants will complete. 

#### About the Evaluation  

Explain exactly what you are evaluating, why, and how.  

- **Elements to be evaluated**. Broadly define the scope of the evaluation—what will be evaluated and what will not be. You may wish to break a large list of activities into sections—front-end, formative, and summative. It is important to make sure your most important stakeholders are completely on board with this part of the plan—especially the things you will leave out—so that there is no confusion or disappointment later.  
- **Evaluation questions**. Lay out the precise evaluation questions you developed earlier in this module. They should flow naturally from what you discussed in the previous section of the evaluation plan.  
- **Indicators**. Describe the indicators you will be looking at, and explain how they relate to your outcomes.  
- **Data sources for evaluation**. Finally, describe the data sources you will draw from and how they relate to your indicators.  

#### Methodology 

This is the “nitty gritty” section that lays out the details of how you will collect your data.  

- **Data collection strategy**. Describe how your data will be collected, by whom, and from whom. If there was any uncertainty or dispute about data collection in your planning process, explain why you made the choices you did. Will you be conducting formative assessments throughout the course of the project? Include that information here.  
- **Evaluation tools to use**. Describe any instruments or resources you will use for data collection. If you have created custom instruments (interview protocols, etc.), you can describe them here, and include them as appendices to the evaluation plan. 
- **Reporting strategy**. How will you communicate the results to your stakeholders? Will you use different strategies for different audiences? What elements will be included for each group? For instance, youth participants probably don’t care whether your partnership project was within budget, but they might like to know how many of their peers participated. This information could be disseminated through a display in your library, a newsletter, or on the library’s social media accounts.
<br>
<br>

<a name="fn4">4</a>:  --https://www.youtube.com/watch?v=WXbkeFIEN8Y&feature=youtu.be 5:15 
<br> 
<a name="fn5">5</a>:  -- Surrounded by Science: Learning Science in Informal Environments. (2010). Washington, D.C.: National Academies Press. https://doi.org/10.17226/12614 p 111
<br> 
<a name="fn6">6</a>:  -- Bonney, R., Ellenbogen, K., Goodyear, L., & Hellenga, R. (Eds.). (2001). Principal investigator’s guide: Managing evaluation in informal STEM education projects. Washington, D.C.: Center for Advancement of Informal Science Education. Retrieved from http://www.informalscience.org/evaluation/pi-guide (p3)
<br> 
<a name="fn7">7</a>:  -- Bonney, R., Ellenbogen, K., Goodyear, L., & Hellenga, R. (Eds.). (2001). Principal investigator’s guide: Managing evaluation in informal STEM education projects. Washington, D.C.: Center for Advancement of Informal Science Education. Retrieved from http://www.informalscience.org/evaluation/pi-guide (p4)
<br> 
<a name="fn8">8</a>:  -- https://youtu.be/WXbkeFIEN8Y 28:40
<br> 
<a name="fn9">9</a>:  -- http://search.credoreference.com/content/entry/sageeval/evaluation_use/
<br> 
<a name="fn10">10</a>:  --Bonney, R., Ellenbogen, K., Goodyear, L., & Hellenga, R. (Eds.). (2001). Principal investigator’s guide: Managing evaluation in informal STEM education projects. Washington, D.C.: Center for Advancement of Informal Science Education. Retrieved from http://www.informalscience.org/evaluation/pi-guide (p13)
<a name="fn11">11</a>:  -- PI Guide pg. 51
<br> 
<a name="fn12">12</a>:  -- Bonney, R., Ellenbogen, K., Goodyear, L., & Hellenga, R. (Eds.). (2001). Principal investigator’s guide: Managing evaluation in informal STEM education projects. Washington, D.C.: Center for Advancement of Informal Science Education. Retrieved from http://www.informalscience.org/evaluation/pi-guide (p16).
<br> 
<a name="fn13">13</a>:  -- Westat, J. F. (2010). The 2010 user-friendly handbook for project evaluation. Washington, D.C.: National Science Foundation. Retrieved from http://www.informalscience.org/sites/default/files/TheUserFriendlyGuide.pdf pp8-9
<br> 
<a name="fn14">14</a>:  -- PI guide p 51
<br> 


